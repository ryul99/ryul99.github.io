<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://torch.vision/feed.xml" rel="self" type="application/atom+xml" /><link href="http://torch.vision/" rel="alternate" type="text/html" /><updated>2020-02-26T18:02:45+09:00</updated><id>http://torch.vision/feed.xml</id><title type="html">Lab of ryul99</title><subtitle>Lab for doing what ryul99 wants
</subtitle><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><entry><title type="html">Pix2Pix paper review</title><link href="http://torch.vision/2020/02/26/Pix2Pix_Paper_Review.html" rel="alternate" type="text/html" title="Pix2Pix paper review" /><published>2020-02-26T00:00:00+09:00</published><updated>2020-02-26T00:00:00+09:00</updated><id>http://torch.vision/2020/02/26/Pix2Pix_Paper_Review</id><content type="html" xml:base="http://torch.vision/2020/02/26/Pix2Pix_Paper_Review.html">&lt;h1 id=&quot;image-to-image-translation-with-conditional-adversarial-networks&quot;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Pix2Pix_Paper_Review/cGAN_method_v3-1.png&quot; alt=&quot;Pix2Pix_Paper_Review/cGAN_method_v3-1.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.07004&quot;&gt;arxiv paper link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;First general purpose conditional GAN for image to image translation task
    &lt;ul&gt;
      &lt;li&gt;Impressive output on inpainting, future state prediction, image manipulation guided by user constraints, style transfer, super-resolution&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;U-net based architecture&lt;/li&gt;
  &lt;li&gt;PatchGAN classifier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generator &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; of conditional GAN is denoted as &lt;script type=&quot;math/tex&quot;&gt;G: \{x,z\} \rightarrow y&lt;/script&gt; when &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is conditional input and &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; is random vector&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;objective&lt;/h2&gt;

&lt;p&gt;conditional GAN objective can be denoted as followings:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_{cGAN} (G,D) = \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{x,z}[\log (1-D(x,G(x,z))]&lt;/script&gt;

&lt;p&gt;In this way, Generator and Discriminator both get conditional input.&lt;/p&gt;

&lt;p&gt;As previous approaches show that mixing GAN objective with traditional loss, authors of this paper also used L1 distance too.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[{\lVert {y-G(x,z)} \rVert} _1]&lt;/script&gt;

&lt;p&gt;So, final objective is derived as followings:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G^* = \arg\min_G\max_D \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G).&lt;/script&gt;

&lt;h3 id=&quot;noise-z&quot;&gt;noise &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;Without &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, the generator could learn deterministic outputs because the generator is only conditioned by conditional input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;But the authors say that they found adding Gaussian noise &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is not effective, as the network just ignores noise &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So they used dropout to some layers to provide noise. Despite the dropout noise, There is only minor stochasticity in the output of the network. In this paper, the way that makes conditional GANs produce highly stochastic output is not provided&lt;/p&gt;

&lt;h2 id=&quot;network-architectures&quot;&gt;Network Architectures&lt;/h2&gt;

&lt;p&gt;Both generator and discriminator use modules of the form convolution-BatchNorm-ReLU&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Pix2Pix_Paper_Review/generator_arch_diagrams_v2-1.png&quot; alt=&quot;Pix2Pix_Paper_Review/generator_arch_diagrams_v2-1.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generator: U-net shape Encoder / Decoder and skip connection&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;markovian-discriminator-patchgan&quot;&gt;Markovian discriminator (PatchGAN)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Pix2Pix_Paper_Review/Untitled.png&quot; alt=&quot;Pix2Pix_Paper_Review/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In image generation task, Generator with L2 loss or L1 loss can produce blurry outputs. In other words, L2, L1 loss make model concentrate on low frequency area and this concentration can fail on high frequency crispness. So, model with L1/L2 loss, produces blurry Image.&lt;/p&gt;

&lt;p&gt;This paper used both L1 loss and GAN discriminator loss. They entrust correcting at low frequencies to L1 loss and correcting at high frequencies to discriminator loss. For high frequency area, it is sufficient to pay attention to the local image patch. So the authors made “PatchGAN” in which discriminator only classify whether small image patch looks like real or not. Model penalize high frequency area by using PatchGAN convolutionally and averaging all PatchGAN’s penalties. It is demonstrated that the size of the patch is much smaller than a full image.&lt;/p&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="paper" /><category term="review" /><category term="DeepLearning" /><category term="Vision" /><summary type="html">Image-to-Image Translation with Conditional Adversarial Networks arxiv paper link First general purpose conditional GAN for image to image translation task Impressive output on inpainting, future state prediction, image manipulation guided by user constraints, style transfer, super-resolution Method U-net based architecture PatchGAN classifier Generator of conditional GAN is denoted as when is conditional input and is random vector objective conditional GAN objective can be denoted as followings: In this way, Generator and Discriminator both get conditional input. As previous approaches show that mixing GAN objective with traditional loss, authors of this paper also used L1 distance too. So, final objective is derived as followings: noise Without , the generator could learn deterministic outputs because the generator is only conditioned by conditional input . But the authors say that they found adding Gaussian noise to is not effective, as the network just ignores noise . So they used dropout to some layers to provide noise. Despite the dropout noise, There is only minor stochasticity in the output of the network. In this paper, the way that makes conditional GANs produce highly stochastic output is not provided Network Architectures Both generator and discriminator use modules of the form convolution-BatchNorm-ReLU Generator: U-net shape Encoder / Decoder and skip connection Markovian discriminator (PatchGAN) In image generation task, Generator with L2 loss or L1 loss can produce blurry outputs. In other words, L2, L1 loss make model concentrate on low frequency area and this concentration can fail on high frequency crispness. So, model with L1/L2 loss, produces blurry Image. This paper used both L1 loss and GAN discriminator loss. They entrust correcting at low frequencies to L1 loss and correcting at high frequencies to discriminator loss. For high frequency area, it is sufficient to pay attention to the local image patch. So the authors made “PatchGAN” in which discriminator only classify whether small image patch looks like real or not. Model penalize high frequency area by using PatchGAN convolutionally and averaging all PatchGAN’s penalties. It is demonstrated that the size of the patch is much smaller than a full image.</summary></entry><entry><title type="html">Basic Deep Learning Concepts good for you</title><link href="http://torch.vision/2020/01/29/Basic_Deep_Learning_Concepts_good_for_you.html" rel="alternate" type="text/html" title="Basic Deep Learning Concepts good for you" /><published>2020-01-29T00:00:00+09:00</published><updated>2020-01-29T00:00:00+09:00</updated><id>http://torch.vision/2020/01/29/Basic_Deep_Learning_Concepts_good_for_you</id><content type="html" xml:base="http://torch.vision/2020/01/29/Basic_Deep_Learning_Concepts_good_for_you.html">&lt;h1 id=&quot;basic-deep-learning-concepts&quot;&gt;Basic Deep Learning Concepts&lt;/h1&gt;

&lt;p&gt;Hard concepts are &lt;strong&gt;Bolded&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised Learning / Unsupervised Learning / &lt;strong&gt;semi-supervised, weakly-supervised&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;weight initialization&lt;/li&gt;
  &lt;li&gt;learning rate decay&lt;/li&gt;
  &lt;li&gt;dropout&lt;/li&gt;
  &lt;li&gt;forward propagation(inference) / backward propagation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;activation&quot;&gt;Activation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;What is activation layer and why use it&lt;/li&gt;
  &lt;li&gt;ReLU, Leaky ReLU&lt;/li&gt;
  &lt;li&gt;softmax&lt;/li&gt;
  &lt;li&gt;sigmoid&lt;/li&gt;
  &lt;li&gt;Difference of softmax and sigmoid&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;loss&quot;&gt;Loss&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;What is loss and why use it&lt;/li&gt;
  &lt;li&gt;L1 Loss, L2 Loss(=MSE Loss)&lt;/li&gt;
  &lt;li&gt;binary cross entropy&lt;/li&gt;
  &lt;li&gt;cross entropy&lt;/li&gt;
  &lt;li&gt;Difference of binary cross entropy and cross entropy&lt;/li&gt;
  &lt;li&gt;Why use binary cross entropy with sigmoid, cross entropy with softmax&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;networks&quot;&gt;Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CNN (Convolution Neural Networks)
    &lt;ul&gt;
      &lt;li&gt;deconvolution layer (transpose convolution)&lt;/li&gt;
      &lt;li&gt;dilated convolution&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN (Recurrent Neural Networks)&lt;/li&gt;
  &lt;li&gt;residual connection&lt;/li&gt;
  &lt;li&gt;U-net&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;train-with-less-data&quot;&gt;Train with less data&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;data augmentation&lt;/li&gt;
  &lt;li&gt;transfer learning&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;semi-supervised, weakly-supervised&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;domain adaptation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;normalization&quot;&gt;Normalization&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Basic%20Deep%20Learning%20Concepts%20good%20for%20you/Untitled.png&quot; alt=&quot;Basic%20Deep%20Learning%20Concepts%20good%20for%20you/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;batch normalization&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Layer Norm&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Instance Norm&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Group Norm&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;sites&quot;&gt;Sites&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0&quot;&gt;Up-sampling with Transposed Convolution&lt;/a&gt; (en)
    &lt;ul&gt;
      &lt;li&gt;korean translate: &lt;a href=&quot;https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/&quot;&gt;https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d&quot;&gt;An Introduction to different Types of Convolutions in Deep Learning&lt;/a&gt; (en)
    &lt;ul&gt;
      &lt;li&gt;korean translate: &lt;a href=&quot;https://zzsza.github.io/data/2018/02/23/introduction-convolution/&quot;&gt;https://zzsza.github.io/data/2018/02/23/introduction-convolution/&lt;/a&gt; (kor)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.07285.pdf&quot;&gt;A guide to convolution arithmetic for deeplearning&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;Attention? Attention!&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/billion-scale-semi-supervised-learning/&quot;&gt;Billion-scale semi-supervised learning for state-of-the-art image and video classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.shurain.net/personal-perspective/n-dimensional-space/&quot;&gt;N-Dimensional Space&lt;/a&gt; (kor)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms/424127#424127&quot;&gt;key, query, values in attention&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/&quot;&gt;Neural Networks, Manifolds, and Topology&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gomguard.tistory.com/187&quot;&gt;blog post about Optimizer&lt;/a&gt; (kor)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://d2l.ai/&quot;&gt;d2l textbook&lt;/a&gt; (en)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://ko.d2l.ai/&quot;&gt;d2l korean textbook&lt;/a&gt; (kor)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;lectures&quot;&gt;Lectures&lt;/h1&gt;

&lt;h2 id=&quot;cs231n&quot;&gt;cs231n&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs231n.stanford.edu/&quot;&gt;http://cs231n.stanford.edu/&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&quot;&gt;https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/&quot;&gt;https://cs231n.github.io/&lt;/a&gt; (en)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aikorea.org/cs231n/&quot;&gt;http://aikorea.org/cs231n/&lt;/a&gt; (kor)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;모두를-위한-딥러닝&quot;&gt;모두를 위한 딥러닝&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hunkim.github.io/ml/&quot;&gt;https://hunkim.github.io/ml/&lt;/a&gt; (kor)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="DeepLearning" /><summary type="html">Basic Deep Learning Concepts Hard concepts are Bolded Supervised Learning / Unsupervised Learning / semi-supervised, weakly-supervised weight initialization learning rate decay dropout forward propagation(inference) / backward propagation Activation What is activation layer and why use it ReLU, Leaky ReLU softmax sigmoid Difference of softmax and sigmoid Loss What is loss and why use it L1 Loss, L2 Loss(=MSE Loss) binary cross entropy cross entropy Difference of binary cross entropy and cross entropy Why use binary cross entropy with sigmoid, cross entropy with softmax Networks CNN (Convolution Neural Networks) deconvolution layer (transpose convolution) dilated convolution RNN (Recurrent Neural Networks) residual connection U-net Train with less data data augmentation transfer learning semi-supervised, weakly-supervised domain adaptation Normalization batch normalization Layer Norm Instance Norm Group Norm Sites Up-sampling with Transposed Convolution (en) korean translate: https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/ An Introduction to different Types of Convolutions in Deep Learning (en) korean translate: https://zzsza.github.io/data/2018/02/23/introduction-convolution/ (kor) A guide to convolution arithmetic for deeplearning (en) Attention? Attention! (en) Billion-scale semi-supervised learning for state-of-the-art image and video classification N-Dimensional Space (kor) key, query, values in attention (en) Neural Networks, Manifolds, and Topology (en) blog post about Optimizer (kor) d2l textbook (en) d2l korean textbook (kor) Lectures cs231n http://cs231n.stanford.edu/ (en) https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv (en) https://cs231n.github.io/ (en) http://aikorea.org/cs231n/ (kor) 모두를 위한 딥러닝 https://hunkim.github.io/ml/ (kor)</summary></entry><entry><title type="html">Efficient Sub-Pixel Convolutional Neural Network</title><link href="http://torch.vision/2020/01/14/Efficient_Sub_Pixel_Convolutional_Neural_Network.html" rel="alternate" type="text/html" title="Efficient Sub-Pixel Convolutional Neural Network" /><published>2020-01-14T00:00:00+09:00</published><updated>2020-01-14T00:00:00+09:00</updated><id>http://torch.vision/2020/01/14/Efficient_Sub_Pixel_Convolutional_Neural_Network</id><content type="html" xml:base="http://torch.vision/2020/01/14/Efficient_Sub_Pixel_Convolutional_Neural_Network.html">&lt;p&gt;Paper arxiv link: &lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;efficient-sub-pixel-convolutional-neural-network-espcn&quot;&gt;Efficient Sub-Pixel Convolutional neural Network (ESPCN)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Efficient_Sub_Pixel_Convolutional_Neural_Network/networkstructure.jpg&quot; alt=&quot;Efficient_Sub_Pixel_Convolutional_Neural_Network/networkstructure.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1. The proposed efficient sub-pixel convolutional neural network (ESPCN), with two convolution layers for feature maps extraction,
and a sub-pixel convolution layer that aggregates the feature maps from LR space and builds the SR image in a single step.&lt;/p&gt;

&lt;h2 id=&quot;contrast-to-previous-works&quot;&gt;Contrast to previous works&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;super-resolve only at the end of the network (efficient sub-pixel convolution layer)
→ Eliminate the need to perform super-resolve in high resolution&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;efficient-sub-pixel-convolution-layer&quot;&gt;Efficient sub-pixel convolution layer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Last layer of ESPCN&lt;/li&gt;
  &lt;li&gt;Rearrange tensor of &lt;script type=&quot;math/tex&quot;&gt;H \times W \times C \cdot r^2&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;rH \times rW \times C&lt;/script&gt; tensor like Figure 1.&lt;/li&gt;
  &lt;li&gt;Implemented at TensorFlow as &lt;a href=&quot;https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/depth-to-space&quot;&gt;depth to space&lt;/a&gt; and PyTorch as &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.PixelShuffle&quot;&gt;PixelShuffle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;advantages&quot;&gt;Advantages&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Upscaling at the last layer.
    &lt;ul&gt;
      &lt;li&gt;Network operations like feature extracting are done at low resolution space. This means that this network needs less computational resource than operating network at high resolution.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learnable &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; upscaling filters
    &lt;ul&gt;
      &lt;li&gt;Network learn &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; upscaling filters for feature maps at last layer rather than one upscaling filter for input image.&lt;/li&gt;
      &lt;li&gt;This layer is not explicit interpolation filter, so network can implicitly learn processing necessary for super resolution.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;→ Network can learn better and more complex low resolution to high resolution mapping than single fixed filter upscaling at first layer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="paper" /><category term="review" /><category term="DeepLearning" /><category term="Vision" /><summary type="html">Paper arxiv link: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network Efficient Sub-Pixel Convolutional neural Network (ESPCN) Figure 1. The proposed efficient sub-pixel convolutional neural network (ESPCN), with two convolution layers for feature maps extraction, and a sub-pixel convolution layer that aggregates the feature maps from LR space and builds the SR image in a single step. Contrast to previous works super-resolve only at the end of the network (efficient sub-pixel convolution layer) → Eliminate the need to perform super-resolve in high resolution Efficient sub-pixel convolution layer Last layer of ESPCN Rearrange tensor of to tensor like Figure 1. Implemented at TensorFlow as depth to space and PyTorch as PixelShuffle Advantages Upscaling at the last layer. Network operations like feature extracting are done at low resolution space. This means that this network needs less computational resource than operating network at high resolution. Learnable upscaling filters Network learn upscaling filters for feature maps at last layer rather than one upscaling filter for input image. This layer is not explicit interpolation filter, so network can implicitly learn processing necessary for super resolution. → Network can learn better and more complex low resolution to high resolution mapping than single fixed filter upscaling at first layer.</summary></entry><entry><title type="html">Deformable Convolution paper review</title><link href="http://torch.vision/2020/01/03/Deformable_Convolutional_Networks.html" rel="alternate" type="text/html" title="Deformable Convolution paper review" /><published>2020-01-03T00:00:00+09:00</published><updated>2020-01-03T00:00:00+09:00</updated><id>http://torch.vision/2020/01/03/Deformable_Convolutional_Networks</id><content type="html" xml:base="http://torch.vision/2020/01/03/Deformable_Convolutional_Networks.html">&lt;h1 id=&quot;deformable-convolutional-networks&quot;&gt;Deformable Convolutional Networks&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.06211&quot;&gt;Paper arxiv link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;deformable-convolution&quot;&gt;Deformable Convolution&lt;/h2&gt;

&lt;p&gt;Standard convolution has fixed sampling location and receptive field. To solve this problem, Deformable convolution use learnable offset.&lt;/p&gt;

&lt;h3 id=&quot;2d-convolution&quot;&gt;2D Convolution&lt;/h3&gt;

&lt;p&gt;The standard 2D convolution consists of two steps: 1) sampling using a regular grid &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt; over the input feature map &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;; 2) summation of sampled values weighted by &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The receptive field size and dilation define the grid &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt;. For example, when 3 X 3 kernel with dilation 1 the grid is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{R}=\{(-1, -1), (-1, 0), \ldots, (0,1), (1, 1)\}&lt;/script&gt;

&lt;p&gt;For each location &lt;script type=&quot;math/tex&quot;&gt;\mathbf{p}_0&lt;/script&gt; on the output feature map &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}&lt;/script&gt;, 2D convolution can be denoted as followings:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y}(\mathbf{p}_0)=\sum_{\mathbf{p}_n\in\mathcal{R}}\mathbf{w}(\mathbf{p}_n)\cdot \mathbf{x}(\mathbf{p}_0+\mathbf{p}_n),&lt;/script&gt;

&lt;h3 id=&quot;2d-deformable-convolution&quot;&gt;2D Deformable Convolution&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Deformable_Convolutional_Networks/deform_conv_layer_v7-1.png&quot; alt=&quot;Deformable_Convolutional_Networks/deform_conv_layer_v7-1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In deformable convolution, the regular grid &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt; is augmented with offsets &lt;script type=&quot;math/tex&quot;&gt;\{\Delta \mathbf{p}_n \lvert n=1,...,N\}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;N= \lvert \mathcal{R} \lvert&lt;/script&gt;. In other words, offsets can be different per grid offset. The offsets are obtained by applying a convolution layer over same input feature map, which means the offsets are learned.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y}(\mathbf{p}_0)=\sum_{\mathbf{p}_n\in\mathcal{R}}\mathbf{w}(\mathbf{p}_n)\cdot \mathbf{x}(\mathbf{p}_0+\mathbf{p}_n+\Delta \mathbf{p}_n).&lt;/script&gt;

&lt;p&gt;Now, the sampling is on the irregular because the offset &lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{p}_n&lt;/script&gt; is typically fractional. So &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}(\mathbf{p}_0+\mathbf{p}_n+\Delta \mathbf{p}_n)&lt;/script&gt; is implemented via bilinear interpolation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}(\mathbf{p})=\sum_\mathbf{q} G(\mathbf{q},\mathbf{p})\cdot \mathbf{x}(\mathbf{q}),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{p}=\mathbf{p}_0+\mathbf{p}_n+\Delta \mathbf{p}_n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}&lt;/script&gt; is integral positions within 2 X 2 square which is centered with &lt;script type=&quot;math/tex&quot;&gt;\mathbf{p}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;G(\cdot,\cdot)&lt;/script&gt; is the bilinear interpolation kernel and can be denoted as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G(\mathbf{q},\mathbf{p})=(1- \lvert q_x-p_x \lvert ) \cdot (1- \lvert q_y-p_y \lvert )&lt;/script&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;As Deformable convolution has offset on its grid, It can have more flexible receptive field.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Standard Convolution&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Deformable Convolution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/Deformable_Convolutional_Networks/standard_conv_receptive_field_v6-1.png&quot; alt=&quot;Deformable_Convolutional_Networks/standard_conv_receptive_field_v6-1.png&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/Deformable_Convolutional_Networks/deform_conv_receptive_field_v6-1.png&quot; alt=&quot;Deformable_Convolutional_Networks/deform_conv_receptive_field_v6-1.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="paper" /><category term="review" /><category term="DeepLearning" /><category term="Vision" /><summary type="html">Deformable Convolutional Networks Paper arxiv link Deformable Convolution Standard convolution has fixed sampling location and receptive field. To solve this problem, Deformable convolution use learnable offset. 2D Convolution The standard 2D convolution consists of two steps: 1) sampling using a regular grid over the input feature map ; 2) summation of sampled values weighted by . The receptive field size and dilation define the grid . For example, when 3 X 3 kernel with dilation 1 the grid is: For each location on the output feature map , 2D convolution can be denoted as followings: 2D Deformable Convolution In deformable convolution, the regular grid is augmented with offsets , where . In other words, offsets can be different per grid offset. The offsets are obtained by applying a convolution layer over same input feature map, which means the offsets are learned. Now, the sampling is on the irregular because the offset is typically fractional. So is implemented via bilinear interpolation as where and is integral positions within 2 X 2 square which is centered with . is the bilinear interpolation kernel and can be denoted as follows: Result As Deformable convolution has offset on its grid, It can have more flexible receptive field. Standard Convolution Deformable Convolution</summary></entry><entry><title type="html">EDVR paper review</title><link href="http://torch.vision/2019/12/24/EDVR_paper_review.html" rel="alternate" type="text/html" title="EDVR paper review" /><published>2019-12-24T00:00:00+09:00</published><updated>2019-12-24T00:00:00+09:00</updated><id>http://torch.vision/2019/12/24/EDVR_paper_review</id><content type="html" xml:base="http://torch.vision/2019/12/24/EDVR_paper_review.html">&lt;h1 id=&quot;edvr-video-restoration-with-enhanced-deformable-convolutional-networks&quot;&gt;EDVR: Video Restoration with Enhanced Deformable Convolutional Networks&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.02716&quot;&gt;Paper arxiv link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;h2 id=&quot;the-overall-framework-of-edvr&quot;&gt;The overall framework of EDVR&lt;/h2&gt;
&lt;p&gt;Given &lt;script type=&quot;math/tex&quot;&gt;2N+1&lt;/script&gt; consecutive frames &lt;script type=&quot;math/tex&quot;&gt;I_{[t-N:t+N]}&lt;/script&gt;, denote middle frame &lt;script type=&quot;math/tex&quot;&gt;I_{t}&lt;/script&gt; as the reference frame and the other frames as neighboring frames&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EDVR_paper_review/overall_structure-1.png&quot; alt=&quot;EDVR_paper_review/overall_structure-1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Inputs with high spatial resolution are first down-sampled to reduce computational cost. Given blurry inputs, a PreDeblur Module is inserted before the PCD Align Module to improve alignment accuracy. We use three input frames as an illustrative example.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Downsampling &amp;amp; Upsampling:&lt;/p&gt;

    &lt;p&gt;If task is not Super Resolution with high spatial resolution inputs, input frames are first downsampled with strided convolution layers.&lt;/p&gt;

    &lt;p&gt;Resize the features back to the original input resolution in the upsampling layer at the end.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PreDeblur:&lt;/p&gt;

    &lt;p&gt;This is used before the alignment module to pre-process blurry inputs and improve alignment accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PCD:&lt;/p&gt;

    &lt;p&gt;Each neighboring frame is aligned to the reference one by the PCD alignment module at the feature level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TSA:&lt;/p&gt;

    &lt;p&gt;The TSA fusion module fuses image information of different frames.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reconstruction Module:&lt;/p&gt;

    &lt;p&gt;The fused features then pass through a reconstruction module, which is a cascade of residual blocks in EDVR and can be easily replaced by any other advanced modules in single image SR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Residual at the end:&lt;/p&gt;

    &lt;p&gt;High-resolution frame is obtained by adding the predicted image residual to a direct upsampled image (Super Resolution).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two-stage strategy:&lt;/p&gt;

    &lt;p&gt;Cascade the same EDVR network but with shallower depth to refine the output frames of the first stage.&lt;/p&gt;

    &lt;p&gt;The cascaded network can further remove the severe motion blur.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;alignment-with-pyramid-cascading-and-deformable-convolution-pcd&quot;&gt;Alignment with Pyramid, Cascading and Deformable Convolution (PCD)&lt;/h1&gt;

&lt;h2 id=&quot;use-of-deformable-convolution&quot;&gt;Use of Deformable Convolution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Alignment features of each neighboring frame to reference frame.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Different from optical-flow based method, deformable alignment is applied on the features of each frame, denote by &lt;script type=&quot;math/tex&quot;&gt;F_{t+i},i \in [{-}N{:}{+}N]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Learnable offset (&lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{P}_{t+i}&lt;/script&gt;) of Deformable Convolution is predicted as &lt;script type=&quot;math/tex&quot;&gt;f(\ [F_{t+i}, F_t]\ )&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is general function consisting several convolution layers and &lt;script type=&quot;math/tex&quot;&gt;[F_{t+i}, F_t]&lt;/script&gt; is concat of two feature &lt;script type=&quot;math/tex&quot;&gt;F_{t+i}, F_t&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pyramidal-processing-and-cascading-refinement&quot;&gt;Pyramidal processing and Cascading refinement&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EDVR_paper_review/pcd_align-1.png&quot; alt=&quot;EDVR_paper_review/pcd_align-1.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Black dash lines:
    &lt;ul&gt;
      &lt;li&gt;To generate feature F in l-th level, strided convolution filters are used to downsample the features at the (l-1)-th pyramid level by factor of 2.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Orange lines:
    &lt;ul&gt;
      &lt;li&gt;Concat reference frame with neighboring frame.&lt;/li&gt;
      &lt;li&gt;The offset is made by convolution with concatenation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Purple dash lines:
    &lt;ul&gt;
      &lt;li&gt;At the l-th level, offsets and aligned features are predicted also with X2 bilinear-interpolation-upsampled offsets and aligned features from the upper (l+1)-th level, respectively&lt;/li&gt;
      &lt;li&gt;Output of Deformable Conv (blue line) and upsampled (l+1)-th level (purple dash lines) are mixed by general function with several convolution layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Light purple background:
    &lt;ul&gt;
      &lt;li&gt;Following the pyramid structure, a subsequent deformable alignment is cascaded to further refine the coarsely aligned features.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;fusion-with-temporoal-and-spatial-attention&quot;&gt;Fusion with Temporoal and Spatial Attention&lt;/h1&gt;

&lt;p&gt;Inter-frame temporal relation and intra-frame spatial relation are critical in fusion because&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;different neighboring frames are not equally informative due to occlusion, blurry regions and parallax problems&lt;/li&gt;
  &lt;li&gt;misalignment and unalignment arising from the preceding alignment stage adversely affect the subsequent reconstruction performance&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;→ Temporal and spatial attentions during the fusion process is adopted&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EDVR_paper_review/tsa_fusion-1.png&quot; alt=&quot;EDVR_paper_review/tsa_fusion-1.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Temporal Attention Maps: blue, red, green object of top left.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{t}^{fusion} = \text{Conv}([\text{concat of attention-weighted features from t-N to t+N}])&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="paper" /><category term="review" /><category term="DeepLearning" /><category term="Vision" /><summary type="html">EDVR: Video Restoration with Enhanced Deformable Convolutional Networks Paper arxiv link Overview The overall framework of EDVR Given consecutive frames , denote middle frame as the reference frame and the other frames as neighboring frames Inputs with high spatial resolution are first down-sampled to reduce computational cost. Given blurry inputs, a PreDeblur Module is inserted before the PCD Align Module to improve alignment accuracy. We use three input frames as an illustrative example. Downsampling &amp;amp; Upsampling: If task is not Super Resolution with high spatial resolution inputs, input frames are first downsampled with strided convolution layers. Resize the features back to the original input resolution in the upsampling layer at the end. PreDeblur: This is used before the alignment module to pre-process blurry inputs and improve alignment accuracy. PCD: Each neighboring frame is aligned to the reference one by the PCD alignment module at the feature level. TSA: The TSA fusion module fuses image information of different frames. Reconstruction Module: The fused features then pass through a reconstruction module, which is a cascade of residual blocks in EDVR and can be easily replaced by any other advanced modules in single image SR. Residual at the end: High-resolution frame is obtained by adding the predicted image residual to a direct upsampled image (Super Resolution). Two-stage strategy: Cascade the same EDVR network but with shallower depth to refine the output frames of the first stage. The cascaded network can further remove the severe motion blur. Alignment with Pyramid, Cascading and Deformable Convolution (PCD) Use of Deformable Convolution Alignment features of each neighboring frame to reference frame. Different from optical-flow based method, deformable alignment is applied on the features of each frame, denote by Learnable offset () of Deformable Convolution is predicted as , where is general function consisting several convolution layers and is concat of two feature . Pyramidal processing and Cascading refinement Black dash lines: To generate feature F in l-th level, strided convolution filters are used to downsample the features at the (l-1)-th pyramid level by factor of 2. Orange lines: Concat reference frame with neighboring frame. The offset is made by convolution with concatenation. Purple dash lines: At the l-th level, offsets and aligned features are predicted also with X2 bilinear-interpolation-upsampled offsets and aligned features from the upper (l+1)-th level, respectively Output of Deformable Conv (blue line) and upsampled (l+1)-th level (purple dash lines) are mixed by general function with several convolution layers Light purple background: Following the pyramid structure, a subsequent deformable alignment is cascaded to further refine the coarsely aligned features. Fusion with Temporoal and Spatial Attention Inter-frame temporal relation and intra-frame spatial relation are critical in fusion because different neighboring frames are not equally informative due to occlusion, blurry regions and parallax problems misalignment and unalignment arising from the preceding alignment stage adversely affect the subsequent reconstruction performance → Temporal and spatial attentions during the fusion process is adopted Temporal Attention Maps: blue, red, green object of top left.</summary></entry><entry><title type="html">Difference of locals, globals, vars in python3</title><link href="http://torch.vision/2019/12/13/python_locals-globals-vars.html" rel="alternate" type="text/html" title="Difference of locals, globals, vars in python3" /><published>2019-12-13T00:00:00+09:00</published><updated>2019-12-13T00:00:00+09:00</updated><id>http://torch.vision/2019/12/13/python_locals-globals-vars</id><content type="html" xml:base="http://torch.vision/2019/12/13/python_locals-globals-vars.html">&lt;p&gt;reference: &lt;a href=&quot;https://stackoverflow.com/a/7969953&quot;&gt;https://stackoverflow.com/a/7969953&lt;/a&gt;&lt;br /&gt;
부족한 영어실력으로 해석해서 정리한 글입니다. 오류가 있을 경우 지적해주세요.&lt;/p&gt;

&lt;h1 id=&quot;파이썬-globals-locals-vars의-차이&quot;&gt;파이썬 globals, locals, vars의 차이&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;globals()
    &lt;ul&gt;
      &lt;li&gt;module의 namespace의 dic이를 return한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;locals()
    &lt;ul&gt;
      &lt;li&gt;함수 안에서는 함수의 locals()를 호출한 순간의 namespace의 dict - 실제 namespace를 reflect하지 않음&lt;/li&gt;
      &lt;li&gt;함수 밖에서는 현재의 namespace의 dict - 실제 namespace를 reflect함&lt;/li&gt;
      &lt;li&gt;함수 안에서 locals()가 호출한 순간의 namespace를 들고오고 reflect 하지 않는다는 것은 CPython specific&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;vars()
    &lt;ul&gt;
      &lt;li&gt;인자로 object를 받는데 받은 object의 __dict__를 부르고 이게 없을 경우 그 object의 namespace를 return&lt;/li&gt;
      &lt;li&gt;인자 없이 사용하는 vars()는 locals()랑 같다&lt;/li&gt;
      &lt;li&gt;vars()의 인자로 function을 줄 수 있는데 이 경우도 object로 봐서 function.__dict__를 부르거나 없다면 function의 object의 namespace를 부른다. function의 namespace와 다르게 function object의 namespace임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="python" /><summary type="html">reference: https://stackoverflow.com/a/7969953 부족한 영어실력으로 해석해서 정리한 글입니다. 오류가 있을 경우 지적해주세요. 파이썬 globals, locals, vars의 차이 globals() module의 namespace의 dic이를 return한다. locals() 함수 안에서는 함수의 locals()를 호출한 순간의 namespace의 dict - 실제 namespace를 reflect하지 않음 함수 밖에서는 현재의 namespace의 dict - 실제 namespace를 reflect함 함수 안에서 locals()가 호출한 순간의 namespace를 들고오고 reflect 하지 않는다는 것은 CPython specific vars() 인자로 object를 받는데 받은 object의 __dict__를 부르고 이게 없을 경우 그 object의 namespace를 return 인자 없이 사용하는 vars()는 locals()랑 같다 vars()의 인자로 function을 줄 수 있는데 이 경우도 object로 봐서 function.__dict__를 부르거나 없다면 function의 object의 namespace를 부른다. function의 namespace와 다르게 function object의 namespace임</summary></entry><entry><title type="html">Run dev-server with Google Cloud Shell</title><link href="http://torch.vision/2019/10/22/ghpage_devserver_with_Google-Cloud-Shell.html" rel="alternate" type="text/html" title="Run dev-server with Google Cloud Shell" /><published>2019-10-22T00:00:00+09:00</published><updated>2019-10-22T00:00:00+09:00</updated><id>http://torch.vision/2019/10/22/ghpage_devserver_with_Google-Cloud-Shell</id><content type="html" xml:base="http://torch.vision/2019/10/22/ghpage_devserver_with_Google-Cloud-Shell.html">&lt;p&gt;github page를 블로그로 사용하면서 불편했던 것 중 하나는 네이버, 티스토리 블로그등과는 다르게 웹 브라우저에서 블로그 글을 수정하거나 새롭게 적는 것이 안된다는 점이었습니다. 
또한 블로그에 수정이 필요할 때마다 로컬에 빌드 환경을 구축해야 했고 로컬의 환경이 달라짐에 따라 빌드가 안되는 경우도 종종 생겼습니다. 
그래서 제가 찾아보았던 방법은 클라우드 서비스에 블로그 소스코드와 빌드 환경을 구축하고 여기에 접속해서 블로그 글을 작성하고 수정하는 방식으로 로컬 환경의 제약없이 블로그를 빌드하는 것이었습니다.
그러던 중 제가 찾은 완벽한 대안은 Google Cloud Platform에서 제공하는 Google Cloud Shell입니다.
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;google-cloud-shell의-특징&quot;&gt;Google Cloud Shell의 특징&lt;/h1&gt;
&lt;p&gt;Google Cloud Shell은 다양한 특징이 있는 데 제가 중점적으로 끌렸던 특징을 모아봤습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;웹 브라우저에서 인스턴스에 대한 명령줄 액세스&lt;/li&gt;
  &lt;li&gt;코드 편집기(베타) 기본 제공&lt;/li&gt;
  &lt;li&gt;5GB의 영구 디스크 저장소&lt;/li&gt;
  &lt;li&gt;사전 설치된 Google Cloud SDK 및 기타 도구&lt;/li&gt;
  &lt;li&gt;자바, Go, Python, Node.js, PHP, Ruby, .NET과 같은 언어 지원&lt;/li&gt;
  &lt;li&gt;웹 미리보기 기능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기서 특히 좋은 것은 인스턴스에서 개발서버를 열었을 때 웹 미리보기가 가능한 점, 웹 브라우저에서 인스턴스에 대한 명령줄 액세스가 가능한 점, 웹 브라우저 상에서 코드 편집기를 제공하는 점이었습니다.
웹 브라우저에서 게시글의 수정, 빌드가 가능하고 deploy하기 전 잘 보이는 지 확인까지 별다른 설정없이 가능하기에, 네이버 / 티스토리 블로그 같이 웹 브라우저에서 모든 블로그 관리가 가능해졌습니다.&lt;br /&gt;
다만 조심해야 할 점은 이 인스턴스는 일시적으로 사용이 가능하다는 점입니다. 홈디렉토리를 제외하면 인스턴스는 일정시간 후에 삭제됩니다.
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;시작하기&quot;&gt;시작하기&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://ssh.cloud.google.com/cloudshell&quot;&gt;https://ssh.cloud.google.com/cloudshell&lt;/a&gt;에 접속하셔서 사용하실 수 있습니다.&lt;br /&gt;
웹 미리보기의 경우 아래 사진의 연필 모양(편집기 모양) 오른쪽에 있는 아이콘입니다.&lt;br /&gt;
&lt;img src=&quot;/assets/images/Google-Cloud-Platform/startcloudshell2.png&quot; title=&quot;GCP control bar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://cloud.google.com/shell/docs/&quot;&gt;공식문서&lt;/a&gt;를 참고하시면 더 좋을 듯 합니다.&lt;/p&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="github" /><category term="GCP" /><summary type="html">github page를 블로그로 사용하면서 불편했던 것 중 하나는 네이버, 티스토리 블로그등과는 다르게 웹 브라우저에서 블로그 글을 수정하거나 새롭게 적는 것이 안된다는 점이었습니다. 또한 블로그에 수정이 필요할 때마다 로컬에 빌드 환경을 구축해야 했고 로컬의 환경이 달라짐에 따라 빌드가 안되는 경우도 종종 생겼습니다. 그래서 제가 찾아보았던 방법은 클라우드 서비스에 블로그 소스코드와 빌드 환경을 구축하고 여기에 접속해서 블로그 글을 작성하고 수정하는 방식으로 로컬 환경의 제약없이 블로그를 빌드하는 것이었습니다. 그러던 중 제가 찾은 완벽한 대안은 Google Cloud Platform에서 제공하는 Google Cloud Shell입니다. Google Cloud Shell의 특징 Google Cloud Shell은 다양한 특징이 있는 데 제가 중점적으로 끌렸던 특징을 모아봤습니다. 웹 브라우저에서 인스턴스에 대한 명령줄 액세스 코드 편집기(베타) 기본 제공 5GB의 영구 디스크 저장소 사전 설치된 Google Cloud SDK 및 기타 도구 자바, Go, Python, Node.js, PHP, Ruby, .NET과 같은 언어 지원 웹 미리보기 기능 여기서 특히 좋은 것은 인스턴스에서 개발서버를 열었을 때 웹 미리보기가 가능한 점, 웹 브라우저에서 인스턴스에 대한 명령줄 액세스가 가능한 점, 웹 브라우저 상에서 코드 편집기를 제공하는 점이었습니다. 웹 브라우저에서 게시글의 수정, 빌드가 가능하고 deploy하기 전 잘 보이는 지 확인까지 별다른 설정없이 가능하기에, 네이버 / 티스토리 블로그 같이 웹 브라우저에서 모든 블로그 관리가 가능해졌습니다. 다만 조심해야 할 점은 이 인스턴스는 일시적으로 사용이 가능하다는 점입니다. 홈디렉토리를 제외하면 인스턴스는 일정시간 후에 삭제됩니다. 시작하기 https://ssh.cloud.google.com/cloudshell에 접속하셔서 사용하실 수 있습니다. 웹 미리보기의 경우 아래 사진의 연필 모양(편집기 모양) 오른쪽에 있는 아이콘입니다. 공식문서를 참고하시면 더 좋을 듯 합니다.</summary></entry><entry><title type="html">Jekyll Trouble Shooting</title><link href="http://torch.vision/2019/09/26/Jekyll-trouble-shooting.html" rel="alternate" type="text/html" title="Jekyll Trouble Shooting" /><published>2019-09-26T00:00:00+09:00</published><updated>2019-09-26T00:00:00+09:00</updated><id>http://torch.vision/2019/09/26/Jekyll-trouble-shooting</id><content type="html" xml:base="http://torch.vision/2019/09/26/Jekyll-trouble-shooting.html">&lt;p&gt;깃헙 페이지를 위해 Jekyll을 사용하면서 겪었던 trouble shooting들을 정리해본다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;윈도우에서 한글 카테고리의 포스트가 포스트 목록에는 보이지만 링크를 눌렀을 때 포스트에 접속이 안되고 Not Found가 뜨는 경우
    &lt;ul&gt;
      &lt;li&gt;윈도우용 루비를 설치해서 Jekyll을 사용하면 url에 한글이 들어갈 경우 한글이 깨지기 때문에 url에 한글이 들어가는 페이지는 들어갈 수 없다.인코딩 설정을 해서 해결할 수 있어 보이는데 나는 그냥 WSL을 사용했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;포스트 링크를 통해서는 들어갈 수 있는데 페이지 목록에 뜨지 않는 경우 등의 플러그인 작동 오류
    &lt;ul&gt;
      &lt;li&gt;Jekyll 3.5부터 &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;에 있는 &lt;code class=&quot;highlighter-rouge&quot;&gt;gems&lt;/code&gt;키워드가 &lt;code class=&quot;highlighter-rouge&quot;&gt;plugins&lt;/code&gt;로 바뀌었다. Jekyll 4.0.0에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;gems&lt;/code&gt;로 되어 있는 경우 인식을 못해 플러그인들이 동작하지 않았다. &lt;a href=&quot;https://jekyllrb.com/news/releases/&quot;&gt;지킬 릴리즈 참조&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="Jekyll" /><summary type="html">깃헙 페이지를 위해 Jekyll을 사용하면서 겪었던 trouble shooting들을 정리해본다. 윈도우에서 한글 카테고리의 포스트가 포스트 목록에는 보이지만 링크를 눌렀을 때 포스트에 접속이 안되고 Not Found가 뜨는 경우 윈도우용 루비를 설치해서 Jekyll을 사용하면 url에 한글이 들어갈 경우 한글이 깨지기 때문에 url에 한글이 들어가는 페이지는 들어갈 수 없다.인코딩 설정을 해서 해결할 수 있어 보이는데 나는 그냥 WSL을 사용했다. 포스트 링크를 통해서는 들어갈 수 있는데 페이지 목록에 뜨지 않는 경우 등의 플러그인 작동 오류 Jekyll 3.5부터 _config.yml에 있는 gems키워드가 plugins로 바뀌었다. Jekyll 4.0.0에서는 gems로 되어 있는 경우 인식을 못해 플러그인들이 동작하지 않았다. 지킬 릴리즈 참조</summary></entry><entry><title type="html">Linux cgroups에 대해 알아보자 2</title><link href="http://torch.vision/2019/06/20/cgroups-2.html" rel="alternate" type="text/html" title="Linux cgroups에 대해 알아보자 2" /><published>2019-06-20T00:00:00+09:00</published><updated>2019-06-20T00:00:00+09:00</updated><id>http://torch.vision/2019/06/20/cgroups-2</id><content type="html" xml:base="http://torch.vision/2019/06/20/cgroups-2.html">&lt;h3 id=&quot;목차&quot;&gt;목차&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/category/삽질/cgroups-1/&quot;&gt;Linux cgroups에 대해 알아보자 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/category/삽질/cgroups-2/&quot;&gt;Linux cgroups에 대해 알아보자 2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/category/삽질/cgroups-1/&quot;&gt;지난 포스트&lt;/a&gt;에서 cgroups이 어떤 것이고 기본적으로 어떻게 사용하면 되는지 알아보았습니다. 이번 글에서는 실제로 어떻게 적용할 수 있는지 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;systemd&quot;&gt;systemd&lt;/h1&gt;
&lt;p&gt;cgroups는 기본적으로 systemd에 적용할 수 있습니다. 하지만 제가 필요했던 제한은 유저에게 가해지는 제한이었기 때문에 systemd 유닛에 제한을 가하는 방법은 사용해보지 않았습니다. 자세한 방법은 &lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/sec-modifying_control_groups&quot;&gt;redhat 7의 관련 문서&lt;/a&gt; 혹은 아래의 reference에서 redhat7 부분을 참고하시면 좋을 듯합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;libcgroup&quot;&gt;libcgroup&lt;/h1&gt;
&lt;p&gt;이 친구는 따로 설치를 해줘야 합니다. OS에 맞게 설치를 진행하시면 됩니다. 저의 경우&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;centos: &lt;code class=&quot;highlighter-rouge&quot;&gt;yum install libcgroup libcgroup-tools&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;ubuntu: &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get install cgroup-bin cgroup-lite libcgroup1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;로 설치했습니다.
libcgroup은 다양한 기능이 있지만 이번에 살펴볼 것은 &lt;code class=&quot;highlighter-rouge&quot;&gt;cgconfig.conf&lt;/code&gt; 작성법입니다. 이 파일은 &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/cgconfig.conf&lt;/code&gt;에 있으며 이 파일을 보고 libcgroup이 cgroup을 생성하는 방식입니다. 파일 작성법은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cgroups-2/image1.png&quot; title=&quot;cgconfig.conf&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;controller&amp;gt;&lt;/code&gt;부분은 subsystem을 적어주면 됩니다. RHEL 7 기준 default subsystem들은 이미 &lt;code class=&quot;highlighter-rouge&quot;&gt;/sys/fs/cgroup/&amp;lt;controller_name&amp;gt;&lt;/code&gt;에 마운트 되어 있습니다. 따라서 맨 윗 블록인 mount블록은 작성을 하지 않아도 됩니다. &lt;br /&gt; perm부분은 name이라는 cgroup의 권한을 설정하는 부분입니다. 생략해도 상관없으며 task에 있는 유저들은 task를 추가, 삭제할 수 있고 admin에 있는 유저들은 cgroup설정을 변경 가능합니다. &lt;br /&gt; 마지막으로 원하는 subsystem을 적어준 뒤 &lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch-subsystems_and_tunable_parameters&quot;&gt;이곳&lt;/a&gt;에 있는 subsystem들의 parameter를 원하는 것만 설정해주면 됩니다. &lt;br /&gt; 마지막으로 수정 후에는 cgconfig service를 재시작 해야 합니다.(systemctl restart &lt;서비스 이름=&quot;&quot;&gt;). 서비스 이름은 OS별로 다를 수 있습니다. 실제 예시는 다음과 같습니다.&lt;/서비스&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cgroups-2/image2.png&quot; title=&quot;cgconfig.conf-example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;cgred&quot;&gt;cgred&lt;/h1&gt;
&lt;p&gt;cgred는 만들어진 제한 정책을 유저, 혹은 유저그룹단위로 적용할 수 있게 합니다. 즉, 특정 유저그룹의 모든 혹은 특정 프로세스를 원하는 cgroup에 속하도록 하는 것입니다. &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/cgrules.conf&lt;/code&gt;에 configure파일을 작성해주면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cgroups-2/image3.png&quot; title=&quot;cgrules.conf-example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞에서부터 user / subsystems / cgroup 순으로 작성하면 되고 매칭은 맨 윗줄부터 매칭해서 할당됩니다. user자리의 경우 앞에 @가 붙으면 user group이 됩니다. subsystem에서 *의 경우 모든 subsystem을 의미합니다. %는 윗 줄의 item과 똑같다를 의미합니다. 마찬가지로 수정 후 cgred service를 재시작해야 합니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt&quot;&gt;kernel.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index&quot;&gt;redhat 7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index&quot;&gt;redhat 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="Linux" /><category term="cgroups" /><summary type="html">목차 Linux cgroups에 대해 알아보자 1 Linux cgroups에 대해 알아보자 2 지난 포스트에서 cgroups이 어떤 것이고 기본적으로 어떻게 사용하면 되는지 알아보았습니다. 이번 글에서는 실제로 어떻게 적용할 수 있는지 알아보도록 하겠습니다. systemd cgroups는 기본적으로 systemd에 적용할 수 있습니다. 하지만 제가 필요했던 제한은 유저에게 가해지는 제한이었기 때문에 systemd 유닛에 제한을 가하는 방법은 사용해보지 않았습니다. 자세한 방법은 redhat 7의 관련 문서 혹은 아래의 reference에서 redhat7 부분을 참고하시면 좋을 듯합니다. libcgroup 이 친구는 따로 설치를 해줘야 합니다. OS에 맞게 설치를 진행하시면 됩니다. 저의 경우 centos: yum install libcgroup libcgroup-tools ubuntu: apt-get install cgroup-bin cgroup-lite libcgroup1 로 설치했습니다. libcgroup은 다양한 기능이 있지만 이번에 살펴볼 것은 cgconfig.conf 작성법입니다. 이 파일은 /etc/cgconfig.conf에 있으며 이 파일을 보고 libcgroup이 cgroup을 생성하는 방식입니다. 파일 작성법은 다음과 같습니다. &amp;lt;controller&amp;gt;부분은 subsystem을 적어주면 됩니다. RHEL 7 기준 default subsystem들은 이미 /sys/fs/cgroup/&amp;lt;controller_name&amp;gt;에 마운트 되어 있습니다. 따라서 맨 윗 블록인 mount블록은 작성을 하지 않아도 됩니다. perm부분은 name이라는 cgroup의 권한을 설정하는 부분입니다. 생략해도 상관없으며 task에 있는 유저들은 task를 추가, 삭제할 수 있고 admin에 있는 유저들은 cgroup설정을 변경 가능합니다. 마지막으로 원하는 subsystem을 적어준 뒤 이곳에 있는 subsystem들의 parameter를 원하는 것만 설정해주면 됩니다. 마지막으로 수정 후에는 cgconfig service를 재시작 해야 합니다.(systemctl restart ). 서비스 이름은 OS별로 다를 수 있습니다. 실제 예시는 다음과 같습니다. cgred cgred는 만들어진 제한 정책을 유저, 혹은 유저그룹단위로 적용할 수 있게 합니다. 즉, 특정 유저그룹의 모든 혹은 특정 프로세스를 원하는 cgroup에 속하도록 하는 것입니다. /etc/cgrules.conf에 configure파일을 작성해주면 됩니다. 앞에서부터 user / subsystems / cgroup 순으로 작성하면 되고 매칭은 맨 윗줄부터 매칭해서 할당됩니다. user자리의 경우 앞에 @가 붙으면 user group이 됩니다. subsystem에서 *의 경우 모든 subsystem을 의미합니다. %는 윗 줄의 item과 똑같다를 의미합니다. 마찬가지로 수정 후 cgred service를 재시작해야 합니다. Reference kernel.org redhat 7 redhat 6</summary></entry><entry><title type="html">Linux cgroups에 대해 알아보자 1</title><link href="http://torch.vision/2019/06/18/cgroups-1.html" rel="alternate" type="text/html" title="Linux cgroups에 대해 알아보자 1" /><published>2019-06-18T00:00:00+09:00</published><updated>2019-06-18T00:00:00+09:00</updated><id>http://torch.vision/2019/06/18/cgroups-1</id><content type="html" xml:base="http://torch.vision/2019/06/18/cgroups-1.html">&lt;h3 id=&quot;목차&quot;&gt;목차&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/category/삽질/cgroups-1/&quot;&gt;Linux cgroups에 대해 알아보자 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/category/삽질/cgroups-2/&quot;&gt;Linux cgroups에 대해 알아보자 2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;cgroups&quot;&gt;cgroups?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cgroups&quot;&gt;위키피디아&lt;/a&gt;에서는 cgroups가 다음과 같이 설명되어 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;즉, &lt;strong&gt;프로세스 모음&lt;/strong&gt;에 대해 시스템의 자원을 제한할 수 있는 커널의 기능입니다. 때문에 cgroups는 하나의 프로세스에 대해 제한을 가하는 ulimit와는 다릅니다. cgroups에는 2007년부터 있던 cgroups v1이 있고 커널 4.5에서 처음 등장한 cgroups v2가 있습니다. 이 글에서 다룰 cgroups는 v1입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;주요-용어&quot;&gt;주요 용어&lt;/h1&gt;
&lt;p&gt;cgroups에서 사용되는 주요 단어들이 있습니다. 아래의 설명은 &lt;a href=&quot;https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt&quot;&gt;kernel.org&lt;/a&gt;의 설명을 번역한 것입니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;cgroup&lt;/strong&gt;: task의 집합과 하나 이상의 subsystem들의 parameters 집합이 서로 연관된 형태입니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;subsystem&lt;/strong&gt;: 묶어진 cgroup의 task들을 특정한 형태로 사용하는 모듈입니다. 여기서는 resource controller로서 cgroup당 자원을 관리하는 모듈입니다. &lt;code class=&quot;highlighter-rouge&quot;&gt;lssubsys -am&lt;/code&gt;명령어를 통해 종류와 마운트 지점을 알 수 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;hierarchy&lt;/strong&gt;: 트리 형태로 구성된 cgroup들의 집합입니다. 시스템의 모든 task는 hierarchy 상의 하나의 cgroup에 존재합니다. 가상의 파일시스템으로 구성되어 있습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;cgroups는 process와 유사하게 1. 계층적이고, 2. child가 parent로부터 특정 attributes를 상속받습니다. 하지만 process는 init프로세스를 root로 하는 거대한 하나의 트리지만 cgroups는 하나 이상의 분리된 hierarchy를 가집니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;규칙&quot;&gt;규칙&lt;/h1&gt;
&lt;p&gt;cgroups에는 규칙이 4가지 있습니다. 각 규칙은 다음과 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;규칙-1&quot;&gt;규칙 1&lt;/h3&gt;
&lt;p&gt;각 hierarchy는 하나 이상의 subsystem들을 가질 수 있습니다. 
&lt;img src=&quot;/assets/images/cgroups-1/image1.png&quot; title=&quot;Rule 1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;규칙-2&quot;&gt;규칙 2&lt;/h3&gt;
&lt;p&gt;한 subsystem이 연결하려는 hierarchy들이 이미 다른 subsystem을 가지고 있다면 둘 이상의 다른 hierarchy에 연결할 수 없습니다.(그 중 하나에만 연결할 수 있습니다.) 하지만 연결하려는 hierarchy들이 subsystem으로 자신 하나만 가진다면 여러 hierarchy에 연결할 수 있습니다.
&lt;img src=&quot;/assets/images/cgroups-1/image2.png&quot; title=&quot;Rule 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;규칙-3&quot;&gt;규칙 3&lt;/h3&gt;
&lt;p&gt;각 task들은 여러 cgroup에 속할 수 있지만 한 hierarchy 내에서는 하나의 cgroup에만 속해야 합니다. 또한 모든 system task는 항상 적어도 하나의 cgroup에 속합니다.
&lt;img src=&quot;/assets/images/cgroups-1/image3.png&quot; title=&quot;Rule 3&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;규칙-4&quot;&gt;규칙 4&lt;/h3&gt;
&lt;p&gt;항상 child task는 parent task의 cgroup을 상속받아 초기화됩니다. 하지만 parent와 child는 독립적이기 때문에 이후에 바뀔 수 있습니다.
&lt;img src=&quot;/assets/images/cgroups-1/image4.png&quot; title=&quot;Rule 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;기본적인-사용법&quot;&gt;기본적인 사용법&lt;/h1&gt;
&lt;p&gt;가장 기본적인 사용법은 다음과 같습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mount -t tmpfs &amp;lt;cgroup_root&amp;gt; &amp;lt;path&amp;gt;&lt;/code&gt;: cgroup_root라는 이름으로 path를 tmpfs마운트 합니다. 이곳이 hierarchy들을 담을 장소가 됩니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mount -t cgroup -o &amp;lt;comma_separated_subsystem_names&amp;gt; &amp;lt;path&amp;gt;&lt;/code&gt;: path에 subsystem들을 마운트합니다. path폴더가 하나의 cgroup이 됩니다.&lt;/li&gt;
  &lt;li&gt;cgroup의 폴더 안에 있는 파일 중 task파일 안에 원하는 프로세스 PID를 적어주면 됩니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;하지만 이런 방법으로는 실제 사용하기가 매우 힘듭니다. 다음 게시글에서 실제 사용법을 알아보겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt&quot;&gt;kernel.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index&quot;&gt;redhat 7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/index&quot;&gt;redhat 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Changmin Choi</name><email>cmchoi9901@gmail.com</email></author><category term="Linux" /><category term="cgroups" /><summary type="html">목차 Linux cgroups에 대해 알아보자 1 Linux cgroups에 대해 알아보자 2 cgroups? 위키피디아에서는 cgroups가 다음과 같이 설명되어 있습니다. cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. 즉, 프로세스 모음에 대해 시스템의 자원을 제한할 수 있는 커널의 기능입니다. 때문에 cgroups는 하나의 프로세스에 대해 제한을 가하는 ulimit와는 다릅니다. cgroups에는 2007년부터 있던 cgroups v1이 있고 커널 4.5에서 처음 등장한 cgroups v2가 있습니다. 이 글에서 다룰 cgroups는 v1입니다. 주요 용어 cgroups에서 사용되는 주요 단어들이 있습니다. 아래의 설명은 kernel.org의 설명을 번역한 것입니다. cgroup: task의 집합과 하나 이상의 subsystem들의 parameters 집합이 서로 연관된 형태입니다. subsystem: 묶어진 cgroup의 task들을 특정한 형태로 사용하는 모듈입니다. 여기서는 resource controller로서 cgroup당 자원을 관리하는 모듈입니다. lssubsys -am명령어를 통해 종류와 마운트 지점을 알 수 있습니다. hierarchy: 트리 형태로 구성된 cgroup들의 집합입니다. 시스템의 모든 task는 hierarchy 상의 하나의 cgroup에 존재합니다. 가상의 파일시스템으로 구성되어 있습니다. cgroups는 process와 유사하게 1. 계층적이고, 2. child가 parent로부터 특정 attributes를 상속받습니다. 하지만 process는 init프로세스를 root로 하는 거대한 하나의 트리지만 cgroups는 하나 이상의 분리된 hierarchy를 가집니다. 규칙 cgroups에는 규칙이 4가지 있습니다. 각 규칙은 다음과 같습니다. 규칙 1 각 hierarchy는 하나 이상의 subsystem들을 가질 수 있습니다. 규칙 2 한 subsystem이 연결하려는 hierarchy들이 이미 다른 subsystem을 가지고 있다면 둘 이상의 다른 hierarchy에 연결할 수 없습니다.(그 중 하나에만 연결할 수 있습니다.) 하지만 연결하려는 hierarchy들이 subsystem으로 자신 하나만 가진다면 여러 hierarchy에 연결할 수 있습니다. 규칙 3 각 task들은 여러 cgroup에 속할 수 있지만 한 hierarchy 내에서는 하나의 cgroup에만 속해야 합니다. 또한 모든 system task는 항상 적어도 하나의 cgroup에 속합니다. 규칙 4 항상 child task는 parent task의 cgroup을 상속받아 초기화됩니다. 하지만 parent와 child는 독립적이기 때문에 이후에 바뀔 수 있습니다. 기본적인 사용법 가장 기본적인 사용법은 다음과 같습니다. mount -t tmpfs &amp;lt;cgroup_root&amp;gt; &amp;lt;path&amp;gt;: cgroup_root라는 이름으로 path를 tmpfs마운트 합니다. 이곳이 hierarchy들을 담을 장소가 됩니다. mount -t cgroup -o &amp;lt;comma_separated_subsystem_names&amp;gt; &amp;lt;path&amp;gt;: path에 subsystem들을 마운트합니다. path폴더가 하나의 cgroup이 됩니다. cgroup의 폴더 안에 있는 파일 중 task파일 안에 원하는 프로세스 PID를 적어주면 됩니다. 하지만 이런 방법으로는 실제 사용하기가 매우 힘듭니다. 다음 게시글에서 실제 사용법을 알아보겠습니다. Reference kernel.org redhat 7 redhat 6</summary></entry></feed>