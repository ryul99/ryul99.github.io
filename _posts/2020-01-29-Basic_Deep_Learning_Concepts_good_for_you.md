---
title:  "Basic Deep Learning Concepts good for you"
layout: article
tags:	DeepLearning
key: "Basic_Deep_Learning_Concepts_good_for_you"
---
# Basic Deep Learning Concepts

Hard concepts are **Bolded**

- Supervised Learning / Unsupervised Learning / **semi-supervised, weakly-supervised**
- weight initialization
- learning rate decay
- dropout
- forward propagation(inference) / backward propagation

## Activation

- What is activation layer and why use it
- ReLU, Leaky ReLU
- softmax
- sigmoid
- Difference of softmax and sigmoid

## Loss

- What is loss and why use it
- L1 Loss, L2 Loss(=MSE Loss)
- binary cross entropy
- cross entropy
- Difference of binary cross entropy and cross entropy
- Why use binary cross entropy with sigmoid, cross entropy with softmax

## Networks

- CNN (Convolution Neural Networks)
    - deconvolution layer (transpose convolution)
    - dilated convolution
- RNN (Recurrent Neural Networks)
- residual connection
- U-net

## Train with less data

- data augmentation
- transfer learning
- **semi-supervised, weakly-supervised**
- **domain adaptation**

## Normalization

![Basic%20Deep%20Learning%20Concepts%20good%20for%20you/Untitled.png](/assets/images/Basic%20Deep%20Learning%20Concepts%20good%20for%20you/Untitled.png)

- batch normalization
- **Layer Norm**
- **Instance Norm**
- **Group Norm**

# Sites

- [Up-sampling with Transposed Convolution](https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0) (en)
    - korean translate: [https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/](https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/)
- [An Introduction to different Types of Convolutions in Deep Learning](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d) (en)
    - korean translate: [https://zzsza.github.io/data/2018/02/23/introduction-convolution/](https://zzsza.github.io/data/2018/02/23/introduction-convolution/) (kor)
- [A guide to convolution arithmetic for deeplearning](https://arxiv.org/pdf/1603.07285.pdf) (en)
- [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) (en)
- [Billion-scale semi-supervised learning for state-of-the-art image and video classification](https://ai.facebook.com/blog/billion-scale-semi-supervised-learning/)
- [N-Dimensional Space](http://www.shurain.net/personal-perspective/n-dimensional-space/) (kor)
- [key, query, values in attention](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms/424127#424127) (en)
- [Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) (en)
- [blog post about Optimizer](https://gomguard.tistory.com/187) (kor)
- [d2l textbook](http://d2l.ai/) (en)
    - [d2l korean textbook](http://ko.d2l.ai/) (kor)

# Lectures

## cs231n

- [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/) (en)
- [https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) (en)
- [https://cs231n.github.io/](https://cs231n.github.io/) (en)
- [http://aikorea.org/cs231n/](http://aikorea.org/cs231n/) (kor)

## 모두를 위한 딥러닝

- [https://hunkim.github.io/ml/](https://hunkim.github.io/ml/) (kor)